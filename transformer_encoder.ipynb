{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHq7i7GxCqGDIzYXTHE03a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/animesh-kishore/scratch-space/blob/master/transformer_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "lbdPitCNGQoP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CH-0gCiWFxuX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer # Auto identify tokenizer based on model passed to AutoTokenizer.from_pretrained('model_name')\n",
        "\n",
        "model = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# e.g. usage\n",
        "text = 'Tokenize text is a core concept in NLP'\n",
        "\n",
        "print('Tokenizer vacabulary size: ', tokenizer.vocab_size)\n",
        "print('Max length of input sequence that model can handle: ', tokenizer.model_max_length, 'tokens')\n",
        "\n",
        "tokenized_text = {}\n",
        "tokenized_text['Numerical Token'] = tokenizer(text)['input_ids']\n",
        "tokenized_text['Token'] = tokenizer.convert_ids_to_tokens(tokenizer(text)['input_ids'])\n",
        "pd.DataFrame(tokenized_text).T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "7iObj_Q4HzhV",
        "outputId": "7145bff2-b622-4ee3-d067-153d3e3dd95b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer vacabulary size:  30522\n",
            "Max length of input sequence that model can handle:  512 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    0      1      2     3     4     5     6        7     8   \\\n",
              "Numerical Token    101  19204   4697  3793  2003  1037  4563     4145  1999   \n",
              "Token            [CLS]  token  ##ize  text    is     a  core  concept    in   \n",
              "\n",
              "                    9     10     11  \n",
              "Numerical Token  17953  2361    102  \n",
              "Token               nl   ##p  [SEP]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-89e7764f-9b6b-4df9-a7df-1bfe63d5a86e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Numerical Token</th>\n",
              "      <td>101</td>\n",
              "      <td>19204</td>\n",
              "      <td>4697</td>\n",
              "      <td>3793</td>\n",
              "      <td>2003</td>\n",
              "      <td>1037</td>\n",
              "      <td>4563</td>\n",
              "      <td>4145</td>\n",
              "      <td>1999</td>\n",
              "      <td>17953</td>\n",
              "      <td>2361</td>\n",
              "      <td>102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Token</th>\n",
              "      <td>[CLS]</td>\n",
              "      <td>token</td>\n",
              "      <td>##ize</td>\n",
              "      <td>text</td>\n",
              "      <td>is</td>\n",
              "      <td>a</td>\n",
              "      <td>core</td>\n",
              "      <td>concept</td>\n",
              "      <td>in</td>\n",
              "      <td>nl</td>\n",
              "      <td>##p</td>\n",
              "      <td>[SEP]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89e7764f-9b6b-4df9-a7df-1bfe63d5a86e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-89e7764f-9b6b-4df9-a7df-1bfe63d5a86e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-89e7764f-9b6b-4df9-a7df-1bfe63d5a86e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3fa377d2-be17-448b-b4cd-6b5adb00fda2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3fa377d2-be17-448b-b4cd-6b5adb00fda2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3fa377d2-be17-448b-b4cd-6b5adb00fda2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[CLS]\",\n          101\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"token\",\n          19204\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"##ize\",\n          4697\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"text\",\n          3793\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 4,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"is\",\n          2003\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 5,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"a\",\n          1037\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 6,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"core\",\n          4563\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 7,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"concept\",\n          4145\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 8,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"in\",\n          1999\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 9,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"nl\",\n          17953\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 10,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"##p\",\n          2361\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 11,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[SEP]\",\n          102\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set max token 100, add pad if number of tokens < 100, return pytorch tensor\n",
        "input_squence = tokenizer.encode_plus(text, return_tensors='pt', padding='max_length', truncation=True, max_length=100)['input_ids']\n",
        "input_squence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmBbldvHQaUE",
        "outputId": "b49c2885-64da-435d-a330-a74e7d2ef2bd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  101, 19204,  4697,  3793,  2003,  1037,  4563,  4145,  1999, 17953,\n",
              "          2361,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding"
      ],
      "metadata": {
        "id": "HcXJFM5MT0St"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a config class for each access of attributes\n",
        "class Config:\n",
        "  def __init__(self, config_dict):\n",
        "    self.__dict__.update(config_dict) # self.__dict__ is a special attribute of an object that contains dictionary of all writable attributes of the class object/instance.\n",
        "\n",
        "config = {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'embedding_dimensions': 128,\n",
        "    'max_tokens': 100, # Allow max 100 tokens per input sequences\n",
        "    'num_attention_heads': 8,\n",
        "    'hidden_dropout_prob': 0.3, # Dropout for feed forward network (FFN)\n",
        "    'intermediate_size': 128 * 4, # Number of neurons in hidden layer of feed forward network (FFN)\n",
        "    'num_encoder_layers': 2,\n",
        "}\n",
        "\n",
        "config = Config(config)"
      ],
      "metadata": {
        "id": "82BbdwfzT65I"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    # Create an embedding look-up table with config.vocab_size entries. Each entry is an embedding vector of size config.embedding_dimensions\n",
        "    self.token_embedding = nn.Embedding(num_embeddings=config.vocab_size, embedding_dim=config.embedding_dimensions)\n",
        "\n",
        "  def forward(self, tokenized_sentence):\n",
        "    return self.token_embedding(tokenized_sentence)\n",
        "\n",
        "token_embedding = TokenEmbedding(config)\n",
        "embedding_output = token_embedding(input_squence)\n",
        "\n",
        "print(embedding_output, 'Shape: ', embedding_output.shape) # Enbedding layer output of shape [batch_size, seq_length, embedding_dims]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tElRxiJd-40",
        "outputId": "357efb45-87e8-49aa-c01a-f8a3fa0eeef2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.7954,  0.0402,  0.1097,  ..., -3.1627, -0.8433, -1.0843],\n",
            "         [-1.2236, -0.2956,  1.8014,  ..., -2.8588, -1.3785,  0.7190],\n",
            "         [ 0.9850, -0.9777, -0.5681,  ...,  0.1499, -0.3708, -1.4400],\n",
            "         ...,\n",
            "         [-1.5691,  0.5431,  0.1510,  ...,  0.6697,  0.0867, -1.5468],\n",
            "         [-1.5691,  0.5431,  0.1510,  ...,  0.6697,  0.0867, -1.5468],\n",
            "         [-1.5691,  0.5431,  0.1510,  ...,  0.6697,  0.0867, -1.5468]]],\n",
            "       grad_fn=<EmbeddingBackward0>) Shape:  torch.Size([1, 100, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encodings"
      ],
      "metadata": {
        "id": "YfQCdaGnhz6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "'''\n",
        "PE(position, i) = sin(position/10000^(i/d_model)) for even i\n",
        "PE(position, i) = cos(position/10000^((i-1)/d_model)) for odd i\n",
        "\n",
        "where:\n",
        "d_model = config.embedding_dimensions\n",
        "position = index in seq_len\n",
        "i = index in embedding vector\n",
        "'''\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    pe = torch.zeros(config.max_tokens, config.embedding_dimensions) # shape [100, 128]\n",
        "    position = torch.arange(0, config.max_tokens, dtype=torch.float).unsqueeze(1) # shape [100, 1]\n",
        "    div_term = 1 / (10000 ** (torch.arange(0, config.embedding_dimensions, 2).float()/config.embedding_dimensions))\n",
        "\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    self.pe = pe.unsqueeze(0) # shape [1, 100, 128]\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.pe\n",
        "\n",
        "positional_encoding = PositionalEncoding(config)\n",
        "pos_enc_output = positional_encoding(embedding_output)"
      ],
      "metadata": {
        "id": "AHIWWD7Lh2l8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention"
      ],
      "metadata": {
        "id": "jR17RNMSefVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embed_dim, head_dim):\n",
        "    super().__init__()\n",
        "    self.q = nn.Linear(embed_dim, head_dim) # Q = Embedding @ Wq\n",
        "    self.k = nn.Linear(embed_dim, head_dim) # K = Embedding @ Wk\n",
        "    self.v = nn.Linear(embed_dim, head_dim) # V = Embedding @ Wv\n",
        "\n",
        "  def scaled_dot_product_attention(self, query, key, value):\n",
        "    dim_k = query.size(-1)\n",
        "    scores = torch.bmm(query, key.transpose(1, 2)) / np.sqrt(dim_k) # (Q @ K.T) / sqrt(head_dim). Shape [100, 100]\n",
        "    weights = F.softmax(scores, dim=-1)\n",
        "    return torch.bmm(weights, value) # Shape [100, head_dim]\n",
        "\n",
        "  def forward(self, hidden_state):\n",
        "    return self.scaled_dot_product_attention(self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    embed_dim = config.embedding_dimensions # 128\n",
        "    num_heads = config.num_attention_heads # 8\n",
        "    head_dim = embed_dim // num_heads\n",
        "\n",
        "    self.heads = nn.ModuleList(\n",
        "        [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
        "    )\n",
        "    self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "  def forward(self, hidden_state):\n",
        "    x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
        "    return self.output_linear(x)\n",
        "\n",
        "multihead_attn = MultiHeadAttention(config)\n",
        "atn_output = multihead_attn(pos_enc_output)\n",
        "\n",
        "atn_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFOPpYlGxUJE",
        "outputId": "199dd049-096a-435d-fc52-7cb8513e5b37"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 100, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wrgp1H8G8FLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Residual connection and Layer Normalization"
      ],
      "metadata": {
        "id": "ak5NRFs__tA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer_norm = nn.LayerNorm(config.embedding_dimensions)\n",
        "\n",
        "add_norm_output = layer_norm(pos_enc_output + atn_output)\n",
        "\n",
        "add_norm_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "po0vRFG__02b",
        "outputId": "d9aa39b3-49a2-4535-8687-9e56d240b975"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 100, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feed Forward Network"
      ],
      "metadata": {
        "id": "0GD8y9ZYBBMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(config.embedding_dimensions, config.intermediate_size) # intermediate_size is 128 * 4\n",
        "    self.linear_2 = nn.Linear(config.intermediate_size, config.embedding_dimensions)\n",
        "    self.gelu = nn.GELU()\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear_1(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.linear_2(x)\n",
        "    return self.dropout(x)\n",
        "\n",
        "feed_forward = FeedForward(config)\n",
        "fnn_output = feed_forward(add_norm_output)\n",
        "\n",
        "fnn_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUfF15pyBDZ5",
        "outputId": "d3812690-2b5b-4e14-fab9-e1214c92476b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 100, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Second add and Norm"
      ],
      "metadata": {
        "id": "tJLI8pRrCQmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer_norm = nn.LayerNorm(config.embedding_dimensions)\n",
        "\n",
        "add_norm_output2 = layer_norm(add_norm_output + fnn_output)\n",
        "\n",
        "add_norm_output2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-qTDDUfCNOa",
        "outputId": "8b4714f7-5d6c-4d74-98e6-faff437e4e6b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 100, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}